{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session_11_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_J5yyvNpm6c",
        "outputId": "9f37df65-9f4f-4c10-eb05-c3a999f9445f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DTvvdW1qfLx"
      },
      "source": [
        "**Encoder**:\r\n",
        "*   The words in the input sentences are embedded respectively\r\n",
        "*   Padding is done for each sentence to keep the length uniform even after the convolution\r\n",
        "*   Once after applying the position embeddings, the resulting sum gives us the element-wise vector.\r\n",
        "*   The element-wise vector is passed to the fully connected layer\r\n",
        "*   The dimension is changed when it is passed to the linear layer\r\n",
        "*   Next is the convolution block\r\n",
        "*   By passing the output to the fully connected layer, the capacity of the network is increased\r\n",
        "\r\n",
        "**Decoder**:\r\n",
        "\r\n",
        "*   The decoder receives the combined vector and the conv vector as an input and gets into the conv block\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A-H6cCBOLaf"
      },
      "source": [
        "# Convolutional Seq2Seq\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDQX3IgesyQk"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAw4EP9cOHSB"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "import spacy\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQy7fPWzthm0"
      },
      "source": [
        "Defining the SEEDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PGBbGe9OTR0"
      },
      "source": [
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dD1v5-itqyI"
      },
      "source": [
        "Download Spacy English and German corpus text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yehZL6jOf-8",
        "outputId": "a11b5d05-6e4c-4c5f-ac0b-2658a8e206d1"
      },
      "source": [
        "!python -m spacy download en\r\n",
        "!python -m spacy download de"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp36-none-any.whl size=14907057 sha256=a60e398bf27023645d4404d6e8d586b1c5124feac407c01a0d9ba1ad6e6aaeef\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7rf1t3mc/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASdVbVuxtywD"
      },
      "source": [
        "Load the downloaded English and German corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2YHL1DPOVKD"
      },
      "source": [
        "spacy_de = spacy.load('de')\r\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub0JM2H9t4I9"
      },
      "source": [
        "English and German text tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdIpGUYDOWu1"
      },
      "source": [
        "def tokenize_de(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes German text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\r\n",
        "\r\n",
        "def tokenize_en(text):\r\n",
        "    \"\"\"\r\n",
        "    Tokenizes English text from a string into a list of strings\r\n",
        "    \"\"\"\r\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ymgGPzt-xn"
      },
      "source": [
        "Define the source SRC and target TRG attributes, batch_first = TRUE since CNN expects the batch_size to be first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6SkU5t4OoJy"
      },
      "source": [
        "SRC = Field(tokenize = tokenize_de, \r\n",
        "            init_token = '<sos>', \r\n",
        "            eos_token = '<eos>', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)\r\n",
        "\r\n",
        "TRG = Field(tokenize = tokenize_en, \r\n",
        "            init_token = '<sos>', \r\n",
        "            eos_token = '<eos>', \r\n",
        "            lower = True, \r\n",
        "            batch_first = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNR4ptScucfX"
      },
      "source": [
        "Split Train, Valid and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh30BF32Opfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56c9fec-ea10-4770-aebb-ab3d9b1c51f4"
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \r\n",
        "                                                    fields=(SRC, TRG))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:04<00:00, 294kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 90.8kB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 87.6kB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u9tFxIpuhgJ"
      },
      "source": [
        "Build Source SRC and Target TRG vocabularies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQapMRRGOqrE"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\r\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUTe_35lumuj"
      },
      "source": [
        "Fetch the available device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IG349zyOsIG"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHwpQAqOup1Z"
      },
      "source": [
        "Initialize BATCH_SIZE\r\n",
        "\r\n",
        "Initialize the Bucket iterators for Train, Valid and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ltm3BEOtgH"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "\r\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\r\n",
        "    (train_data, valid_data, test_data), \r\n",
        "     batch_size = BATCH_SIZE,\r\n",
        "     device = device)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8B4w6V8u90H"
      },
      "source": [
        "Two context vectors, one from Conved and the other combined. Both are sent to the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLmlrp8AzF0Z"
      },
      "source": [
        "Convolution is applied on top of the embedded words\r\n",
        "\r\n",
        "For example:\r\n",
        "\r\n",
        "Assume we've embedding dimension of 256 and considering 3 words at a time, the result will be 256 X 3. Kernel used to convolute on top of it is 3 X 3. Matrix multiplication on the dimensions \r\n",
        "\r\n",
        "(256 X 3) * (3 X 1) results in 256 X 1.\r\n",
        "\r\n",
        "Striding over the words and the final output will be 256 X 1 X (Number of Words) after padding\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_dEuyGOwHO"
      },
      "source": [
        "## Encoder\r\n",
        "\r\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/raw/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq1.png)\r\n",
        "\r\n",
        "## Convolutional Blocks\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ov1SsR0Q5h"
      },
      "source": [
        "Encoder Class definition inherited Module class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEjxwGUxOun-"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        \r\n",
        "        #initializing construtor of the super class\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        #asserting kernel size to be an odd number\r\n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "        \r\n",
        "        #embedding input is input dim and it will be converted to embedding dimension\r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\r\n",
        "        #position embedding - this needs to know max length and match it to embedding dimension\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        #adding linear layer and embedding dimension coming in and hidden dimension going out\r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        #this from hidden to embedding\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        #here defining conv block, padding is 1 if kernel size is 3, padding is 2 if kernel size is 5\r\n",
        "        #this is to make sure centre pixel of the kernel is at the first word.\r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size, \r\n",
        "                                              padding = (kernel_size - 1) // 2)\r\n",
        "                                    for _ in range(n_layers)])\r\n",
        "        # we are using dropout also.\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        #create position tensor\r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [0, 1, 2, 3, ..., src len - 1]\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "        \r\n",
        "        #embed tokens and positions\r\n",
        "        tok_embedded = self.tok_embedding(src)\r\n",
        "        pos_embedded = self.pos_embedding(pos)\r\n",
        "        \r\n",
        "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #combine embeddings by elementwise summing\r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #pass embedded through linear layer to convert from emb dim to hid dim\r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #permute for convolutional layer\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch size, hid dim, src len]\r\n",
        "        \r\n",
        "        #begin convolutional blocks...\r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            #pass through convolutional layer\r\n",
        "            conved = conv(self.dropout(conv_input))\r\n",
        "\r\n",
        "            #conved = [batch size, 2 * hid dim, src len]\r\n",
        "\r\n",
        "            #pass through GLU activation function\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, src len]\r\n",
        "            \r\n",
        "            #apply residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, src len]\r\n",
        "            \r\n",
        "            #set conv_input to conved for next loop iteration\r\n",
        "            conv_input = conved\r\n",
        "        \r\n",
        "        #...end convolutional blocks\r\n",
        "        \r\n",
        "        #permute and convert back to emb dim\r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\r\n",
        "        combined = (conved + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        return conved, combined"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUEAan-FPIoJ"
      },
      "source": [
        "## Decoder \r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq3.png)\r\n",
        "\r\n",
        "## Decoder Conv Blocks\r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq4.png)\r\n",
        "\r\n",
        "## Incorrect Padding\r\n",
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/convseq2seq5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtvERJkB00Mp"
      },
      "source": [
        "Decoder receives Conved input and the combined input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSyBwu30PH-2"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 emb_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 kernel_size, \r\n",
        "                 dropout, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device,\r\n",
        "                 max_length = 100):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\r\n",
        "        \r\n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        \r\n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\r\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\r\n",
        "        \r\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \r\n",
        "                                              out_channels = 2 * hid_dim, \r\n",
        "                                              kernel_size = kernel_size)\r\n",
        "                                    for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "      \r\n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #embedded = [batch size, trg len, emb dim]\r\n",
        "        #conved = [batch size, hid dim, trg len]\r\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #permute and convert back to emb dim\r\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #conved_emb = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        combined = (conved_emb + embedded) * self.scale\r\n",
        "        \r\n",
        "        #combined = [batch size, trg len, emb dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\r\n",
        "        \r\n",
        "        #energy = [batch size, trg len, src len]\r\n",
        "        \r\n",
        "        attention = F.softmax(energy, dim=2)\r\n",
        "        \r\n",
        "        #attention = [batch size, trg len, src len]\r\n",
        "            \r\n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch size, trg len, emd dim]\r\n",
        "        \r\n",
        "        #convert from emb dim -> hid dim\r\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\r\n",
        "        \r\n",
        "        #attended_encoding = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #apply residual connection\r\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\r\n",
        "        \r\n",
        "        #attended_combined = [batch size, hid dim, trg len]\r\n",
        "        \r\n",
        "        return attention, attended_combined\r\n",
        "        \r\n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "            \r\n",
        "        #create position tensor\r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "        \r\n",
        "        #embed tokens and positions\r\n",
        "        tok_embedded = self.tok_embedding(trg)\r\n",
        "        pos_embedded = self.pos_embedding(pos)\r\n",
        "        \r\n",
        "        #tok_embedded = [batch size, trg len, emb dim]\r\n",
        "        #pos_embedded = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        #combine embeddings by elementwise summing\r\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\r\n",
        "        \r\n",
        "        #embedded = [batch size, trg len, emb dim]\r\n",
        "        \r\n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\r\n",
        "        conv_input = self.emb2hid(embedded)\r\n",
        "        \r\n",
        "        #conv_input = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #permute for convolutional layer\r\n",
        "        conv_input = conv_input.permute(0, 2, 1) \r\n",
        "        \r\n",
        "        #conv_input = [batch size, hid dim, trg len]\r\n",
        "        \r\n",
        "        batch_size = conv_input.shape[0]\r\n",
        "        hid_dim = conv_input.shape[1]\r\n",
        "        \r\n",
        "        for i, conv in enumerate(self.convs):\r\n",
        "        \r\n",
        "            #apply dropout\r\n",
        "            conv_input = self.dropout(conv_input)\r\n",
        "        \r\n",
        "            #need to pad so decoder can't \"cheat\"\r\n",
        "            padding = torch.zeros(batch_size, \r\n",
        "                                  hid_dim, \r\n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\r\n",
        "                \r\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\r\n",
        "        \r\n",
        "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\r\n",
        "        \r\n",
        "            #pass through convolutional layer\r\n",
        "            conved = conv(padded_conv_input)\r\n",
        "\r\n",
        "            #conved = [batch size, 2 * hid dim, trg len]\r\n",
        "            \r\n",
        "            #pass through GLU activation function\r\n",
        "            conved = F.glu(conved, dim = 1)\r\n",
        "\r\n",
        "            #conved = [batch size, hid dim, trg len]\r\n",
        "            \r\n",
        "            #calculate attention\r\n",
        "            attention, conved = self.calculate_attention(embedded, \r\n",
        "                                                         conved, \r\n",
        "                                                         encoder_conved, \r\n",
        "                                                         encoder_combined)\r\n",
        "            \r\n",
        "            #attention = [batch size, trg len, src len]\r\n",
        "            \r\n",
        "            #apply residual connection\r\n",
        "            conved = (conved + conv_input) * self.scale\r\n",
        "            \r\n",
        "            #conved = [batch size, hid dim, trg len]\r\n",
        "            \r\n",
        "            #set conv_input to conved for next loop iteration\r\n",
        "            conv_input = conved\r\n",
        "            \r\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\r\n",
        "         \r\n",
        "        #conved = [batch size, trg len, emb dim]\r\n",
        "            \r\n",
        "        output = self.fc_out(self.dropout(conved))\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H56wQXdqPb1_"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        \r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\r\n",
        "           \r\n",
        "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\r\n",
        "        #encoder_conved is output from final encoder conv. block\r\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \r\n",
        "        #  positional embeddings \r\n",
        "        encoder_conved, encoder_combined = self.encoder(src)\r\n",
        "            \r\n",
        "        #encoder_conved = [batch size, src len, emb dim]\r\n",
        "        #encoder_combined = [batch size, src len, emb dim]\r\n",
        "        \r\n",
        "        #calculate predictions of next words\r\n",
        "        #output is a batch of predictions for each word in the trg sentence\r\n",
        "        #attention a batch of attention scores across the src sentence for \r\n",
        "        #  each word in the trg sentence\r\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #attention = [batch size, trg len - 1, src len]\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBUcXsz0PdUB"
      },
      "source": [
        "INPUT_DIM = len(SRC.vocab)\r\n",
        "OUTPUT_DIM = len(TRG.vocab)\r\n",
        "EMB_DIM = 256\r\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\r\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\r\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\r\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\r\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\r\n",
        "ENC_DROPOUT = 0.25\r\n",
        "DEC_DROPOUT = 0.25\r\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\r\n",
        "    \r\n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\r\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vPNjyGcPevW",
        "outputId": "0900b666-0445-47ec-dea1-b97d5c4d665f"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 37,351,685 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA11W1cGPgBi"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVexyPAPPiPO"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    for i, batch in enumerate(iterator):\r\n",
        "        \r\n",
        "        src = batch.src\r\n",
        "        trg = batch.trg\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "        \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "        \r\n",
        "        loss = criterion(output, trg)\r\n",
        "        \r\n",
        "        loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-w9g5kZPjhS"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    epoch_loss = 0\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in enumerate(iterator):\r\n",
        "\r\n",
        "            src = batch.src\r\n",
        "            trg = batch.trg\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "\r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhPAu069Pk20"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "FPUxl1tMPmVg",
        "outputId": "4751ccb0-a725-40a1-e69a-6b017f4a9a06"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "CLIP = 0.1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 31s\n",
            "\tTrain Loss: 4.2830383557054965 | Train PPL: 72.46026594597448\n",
            "\t Val. Loss: 3.05972421169281 |  Val. PPL: 21.3216760821445\n",
            "Epoch: 02 | Time: 2m 31s\n",
            "\tTrain Loss: 3.161931313081985 | Train PPL: 23.616162116459638\n",
            "\t Val. Loss: 2.5151054561138153 |  Val. PPL: 12.367912982382006\n",
            "Epoch: 03 | Time: 2m 31s\n",
            "\tTrain Loss: 2.8221624160128016 | Train PPL: 16.813168455603986\n",
            "\t Val. Loss: 2.3086665272712708 |  Val. PPL: 10.06099963741522\n",
            "Epoch: 04 | Time: 2m 31s\n",
            "\tTrain Loss: 2.6980167548561935 | Train PPL: 14.85025081278921\n",
            "\t Val. Loss: 2.258168563246727 |  Val. PPL: 9.565554406857549\n",
            "Epoch: 05 | Time: 2m 30s\n",
            "\tTrain Loss: 2.700475594020625 | Train PPL: 14.886810119397323\n",
            "\t Val. Loss: 2.279975637793541 |  Val. PPL: 9.776442230923575\n",
            "Epoch: 06 | Time: 2m 31s\n",
            "\tTrain Loss: 3.9276706941327335 | Train PPL: 50.788537742391256\n",
            "\t Val. Loss: 82.06579256057739 |  Val. PPL: 4.372409830688941e+35\n",
            "Epoch: 07 | Time: 2m 28s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OverflowError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-eaf718f8088b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\tTrain Loss: {train_loss} | Train PPL: {math.exp(train_loss)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\t Val. Loss: {valid_loss} |  Val. PPL: {math.exp(valid_loss)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOverflowError\u001b[0m: math range error"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gN-GkjB7Pn7H"
      },
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt'))\r\n",
        "\r\n",
        "test_loss = evaluate(model, test_iterator, criterion)\r\n",
        "\r\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MGwOb3qPrS4"
      },
      "source": [
        "def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):\r\n",
        "\r\n",
        "    model.eval()\r\n",
        "        \r\n",
        "    if isinstance(sentence, str):\r\n",
        "        nlp = spacy.load('de')\r\n",
        "        tokens = [token.text.lower() for token in nlp(sentence)]\r\n",
        "    else:\r\n",
        "        tokens = [token.lower() for token in sentence]\r\n",
        "\r\n",
        "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\r\n",
        "        \r\n",
        "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\r\n",
        "\r\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "        encoder_conved, encoder_combined = model.encoder(src_tensor)\r\n",
        "\r\n",
        "    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]\r\n",
        "\r\n",
        "    for i in range(max_len):\r\n",
        "\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, encoder_conved, encoder_combined)\r\n",
        "        \r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        \r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "\r\n",
        "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\r\n",
        "            break\r\n",
        "    \r\n",
        "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\r\n",
        "    \r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2ApPvr9PtgK"
      },
      "source": [
        "def display_attention(sentence, translation, attention):\r\n",
        "    \r\n",
        "    fig = plt.figure(figsize=(10,10))\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "        \r\n",
        "    attention = attention.squeeze(0).cpu().detach().numpy()\r\n",
        "    \r\n",
        "    cax = ax.matshow(attention, cmap='bone')\r\n",
        "   \r\n",
        "    ax.tick_params(labelsize=15)\r\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \r\n",
        "                       rotation=45)\r\n",
        "    ax.set_yticklabels(['']+translation)\r\n",
        "\r\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ48fLMsPu-1"
      },
      "source": [
        "example_idx = 2\r\n",
        "\r\n",
        "src = vars(train_data.examples[example_idx])['src']\r\n",
        "trg = vars(train_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULOvdnc1PwG0"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bU8kjg4PxTq"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADickZvsPyhI"
      },
      "source": [
        "example_idx = 2\r\n",
        "\r\n",
        "src = vars(valid_data.examples[example_idx])['src']\r\n",
        "trg = vars(valid_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkXuD77iP0f6"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uphiAwagP1ks"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndDCH8m6P23M"
      },
      "source": [
        "example_idx = 9\r\n",
        "\r\n",
        "src = vars(test_data.examples[example_idx])['src']\r\n",
        "trg = vars(test_data.examples[example_idx])['trg']\r\n",
        "\r\n",
        "print(f'src = {src}')\r\n",
        "print(f'trg = {trg}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj_clFYtP4E0"
      },
      "source": [
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\r\n",
        "\r\n",
        "print(f'predicted trg = {translation}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVkG0MkOP5QQ"
      },
      "source": [
        "display_attention(src, translation, attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq0kiimuWRdT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}