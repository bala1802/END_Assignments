{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session6_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1oEHjPod783d7Yad9U3_1LXXK5WKnipAh",
      "authorship_tag": "ABX9TyP392TfdrdrDKTmU0VY22/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bala1802/END_Assignments/blob/main/Session6_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv_sdBClH0at"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McpjxTc8IOjb"
      },
      "source": [
        "**Basic Understanding of LSTM, GRU and Attention mechanisms**\n",
        "\n",
        "The variety of sequence to sequence algorithms are introduced due to the famous *vanishing gradient* problem in **RNN**.\n",
        "\n",
        "**LSTM** or Long short term memory has it building block in RNN but they are capable of retrieving the context of sequential data due to cell state. \n",
        "\n",
        "![image](https://github.com/bala1802/END_Assignments/blob/main/LSTM1.jpeg?raw=true)\n",
        "\n",
        "![image](https://github.com/bala1802/END_Assignments/blob/main/LSTM2.jpeg?raw=true)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7a7Ic5YJwTt"
      },
      "source": [
        "Cell state or the $C_{t}$ associates it self with the whole LSTM architecture. The neural nets inside the architecture decide if something is to be forgotten or added to the cell state. The cell state intern help in deciding the output  according to the context saved in it.\n",
        "\n",
        "\n",
        "\n",
        "*   **Forget gate:** The first part of LSTM is a forget gate . The input $h_{t-1}$ and $x$ are given to neural net which decide if anything from the cell has to forgotten. It is passed through a sigmoid activation function. The output of the sigmoid then gets multiplied with the cell state $C_{t-1} $.\n",
        "*   **Addition gate:** Next part is to add things to cell state . Here first we decide what value to update given by $i_{t}$. then we  get $\\tilde{C}_t $ which the state that has to added from a neural network followed by a tanh squashing function. The multiplication of both these units are addded to cell state.\n",
        "\n",
        "*  **Output gate:**  The output gate decides finally what will be the output of the LSTM. Here a neural net output after passing through sigmoid gate activation is multiplied with the output of the cell state given to tanh function. This way an output considering the cell state is determined.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvLJe3vDKm_U"
      },
      "source": [
        "**GRU- Gated Recurrent Unit**\n",
        "\n",
        "After the LSTM a number of algorithm based on LSTM came out . One of them was GRU which is quite popular as it combined the forget and addition gate to one update gate. It also updated the memory twice once with Reset gate and then with the hidden state . It results in a simpler model than LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoVVw7HaK2vL"
      },
      "source": [
        "**Attention Mechanism**\n",
        "\n",
        "Encoder decoder was mainly used for neural machine translation where it was used for translating one language to other.  Attention mechanism is an extension of encoder decoder.\n",
        "\n",
        "In attention we use mainly three units an encoder , a decoder and attention mechanism.\n",
        "\n",
        "In attention we will have input vector which coupled with feedback $s$ will passed through neural network will give $\\alpha$ which when multiplied with inputs vector will give us the context we have to give attention to and will be given to output to predict final output.\n",
        "\n",
        "![image](https://github.com/bala1802/END_Assignments/blob/main/Attention%20Mechanism.jpeg?raw=true)\n",
        "\n",
        "In this let us assume we have input $x_{1}$ , $x_{2}$ , $x_{3}$ , \n",
        "$x_{4}$ which when given to encoder gives  $h_{1}$, $h_{2}$, $h_{3}$, $h_{4}$ .\n",
        "\n",
        "$h_{1}$, $h_{2}$, $h_{3}$, $h_{4}$ along with $s_{0}$ are given to a fully- connected layer , then to a softmax to give  $\\alpha_{1}$, $\\alpha_{2}$, $\\alpha_{3}$, $\\alpha_{4}$ .\n",
        "\n",
        "$\\alpha$ is are know as attention weights and used to calculated the context vector.\n",
        "\n",
        "Now the  $\\alpha_{1}$, $\\alpha_{2}$, $\\alpha_{3}$, $\\alpha_{4}$ are multiplied with $h_{1}$, $h_{2}$, $h_{3}$, $h_{4}$ to give $c_{1}$ . \n",
        "$c_{1}$ is the context . Next the context along with $s_{0}$ is given to decoder and a $s_{1}$ or $y_{1}$ is predicted . $s_{1}$ is feedback to get concatenated with  $h_{1}$, $h_{2}$, $h_{3}$, $h_{4}$  and so forth we make next predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm0DoKZhJNcd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}